{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 docs\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_files = [\"./data/2311.09277.pdf\"]\n",
    ")\n",
    "\n",
    "docs = reader.load_data()\n",
    "\n",
    "print(f\"Loaded {len(docs)} docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianbraun/Google Drive/py_llama3-ollama-rag/.llama3-ollama-rag/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You try to use a model that was created with version 2.7.0.dev0, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding( model_name=\"Snowflake/snowflake-arctic-embed-m\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "index = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'llama_index.core.query_engine' has no attribute 'update_prompts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m      4\u001b[0m qa_prompt_tmpl_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext information is below. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m qa_prompt_tmpl \u001b[38;5;241m=\u001b[39m PromptTemplate(qa_prompt_tmpl_str)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_prompts\u001b[49m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_syntheziser:text_qa_template\u001b[39m\u001b[38;5;124m\"\u001b[39m: qa_prompt_tmpl})\n\u001b[1;32m     20\u001b[0m response \u001b[38;5;241m=\u001b[39m query_engine\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhat is this repository about?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'llama_index.core.query_engine' has no attribute 'update_prompts'"
     ]
    }
   ],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core import query_engine\n",
    "\n",
    "qa_prompt_tmpl_str = (\n",
    "    \"Context information is below. \\n\"\n",
    "    \"-------------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"-------------------------\\n\"\n",
    "    \"Given the context information above I want you \\n\"\n",
    "    \"to think step by step to answer the query in a crisp \\n\"\n",
    "    \" manner, incase case you don't know the answer say \\n\"\n",
    "    \"'I don't know!'. \\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "query_engine.update_prompts({\"response_syntheziser:text_qa_template\": qa_prompt_tmpl})\n",
    "\n",
    "response = query_engine.query('What is this repository about?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open To-Dos\n",
    "\n",
    "Source: https://twitter.com/akshay_pachaar/status/1781299290004189362?s=12\n",
    "\n",
    "- Fixing the query_engine issue \n",
    "- adding ollama as LLm Engine with LLama3\n",
    "- Building small Frontend with open-web-ui or streamlit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llama3-ollama-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
